{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with LSTM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Dependencies + Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "%store -r initial_processing_df\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "#from keras.models import Sequential\n",
    "#from keras.layers import Dense, Dropout, Activation, SimpleRNN, Embedding, LSTM\n",
    "#from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "#from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import AUC\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from tqdm import tqdm\n",
    "#from textblob import TextBlob\n",
    "#from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import ngrams\n",
    "from nltk.sentiment import vader\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import keras\n",
    "#import random\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "np.random.seed(124)\n",
    "tf.random.set_seed(124)\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "df = pd.read_csv('../Examensarbete_DS21/rnn_hotel_training_data.csv', encoding='ISO-8859-1')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's have a look at our dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only need the review text and raiting\n",
    "df = df[['Review Text', 'Review Rating']]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for null values.\n",
    "df.isna().any()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positive Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If rating [1, 2, 3] = Negative and if rating [4, 5] = Positive\n",
    "def ratings(rating):\n",
    "    if rating>0 and rating<=3:\n",
    "        return 0\n",
    "    if rating>3 and rating<=5:\n",
    "        return 1\n",
    "    \n",
    "df['Positive'] = df['Review Rating'].apply(ratings)\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Count Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count(review):\n",
    "    review_list = review.split()\n",
    "    return len(review_list)\n",
    "\n",
    "df['Word_count'] = df['Review Text'].apply(word_count)\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK Text Processing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_text(sen):\n",
    "    '''Cleans up text data, leaving 2 or more char long non-stopwords containing A-Z & a-z only in lowercase'''\n",
    "\n",
    "    sentence = sen.lower()\n",
    "    \n",
    "    # Remove punctuations and numbers\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "    \n",
    "    # Remove single characters that are not part of words with more than one character\n",
    "    sentence = re.sub(r\"\\b[a-zA-Z]\\b\", '', sentence)\n",
    "    \n",
    "    # Remove multiple spaces\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = sentence.split()\n",
    "    filtered_words = [word for word in words if not word in stop_words]\n",
    "    \n",
    "    return ' '.join(filtered_words)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count of Reviews by Stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Review Rating'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df['Review Rating'].value_counts().sort_index().plot(\n",
    "    kind='bar',\n",
    "    title='Count of Reviews by Stars',\n",
    "    figsize=(8, 4)\n",
    ")\n",
    "ax.set_xlabel('Review Stars')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset consists of a significant number of positive reviews."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count of Reviews by Positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Positive'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df['Positive'].value_counts().sort_index().plot(\n",
    "    kind='bar',\n",
    "    title='Count of Reviews by Positive',\n",
    "    figsize=(6, 4)\n",
    ")\n",
    "ax.set_xlabel('Positive')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of data is highly skewed towards positive reviews."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Count by Review Rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.barplot(df, x='Review Rating', y='Word_count')\n",
    "ax.set_title('Word count by Review Rating')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the graph, it can be inferred that the number of words decreases as the star rating increases. This is not surprising, as people tend to have more to say when they are dissatisfied."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Frequency\n",
    "\n",
    "### Corpus\n",
    "To analyze the most frequent words in the reviews, we will create a function called corpus(). This function will convert the review text into a list of words. However, before doing so, we need to clean the text using the preprocess_text function created earlier using the NLTK library. The cleaning process includes removing punctuations, numbers, single characters, multiple spaces, and stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty list where the cleaned text will be stored\n",
    "clean_text = []\n",
    "\n",
    "clean_sentences = list(df['Review Text'])\n",
    "for sen in tqdm(clean_sentences):\n",
    "    clean_text.append(preprocess_text(sen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that takes the review text and splits all the words\n",
    "def corpus(text):\n",
    "    test_list = text.split()\n",
    "    return test_list\n",
    "\n",
    "# Convert the clean_text list to a pandas Series object\n",
    "clean_text_series = pd.Series(clean_text)\n",
    "\n",
    "# Applies the corpus function on the review text and then saves into a new column\n",
    "df['Corpus_Reviews'] = clean_text_series.apply(corpus)\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequently occuring N_grams\n",
    "Most frequently occurring n-grams\n",
    "What is an n-gram? An n-gram is a sequence of n words in a text. Most words by themselves may not present the entire context. Typically, adverbs such as 'most' or 'very' are used to modify verbs and adjectives. Therefore, n-grams help analyze phrases and not just words, which can lead to better insights.\n",
    "\n",
    "Some examples of n-grams are:\n",
    "\n",
    "- bi-gram, which means two words in a sequence, such as 'very good' or 'too great'.\n",
    "- tri-gram, which means three words in a sequence, such as 'how was your day'. This would be broken down to 'how was your' and 'was your day'.\n",
    "- quad-gram, which means four words in a sequence.\n",
    "- quint-gram, which means five words in a sequence."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 15 Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to store all words in the corpus\n",
    "all_words = []\n",
    "\n",
    "# Iterate over each review in the corpus and add all words to the all_words list\n",
    "for review in df['Corpus_Reviews']:\n",
    "    all_words.extend(review)\n",
    "    \n",
    "# Count the frequency of each word and get the 10 most common words\n",
    "word_freq = Counter(all_words)\n",
    "top_words = word_freq.most_common(15)\n",
    "\n",
    "# Print the 10 most common words\n",
    "print(\"The 10 most frequent words in the corpus are:\")\n",
    "for word, freq in top_words:\n",
    "    print(f\"{word}: {freq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = Counter(all_words)\n",
    "top_words = word_freq.most_common(15)\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "ax = sns.barplot(x=[val[1] for val in top_words], y=[val[0] for val in top_words], color='r')\n",
    "ax.set_xlabel('Frequency')\n",
    "ax.set_ylabel('Word')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to store all bi-grams in the corpus\n",
    "all_bigrams = []\n",
    "\n",
    "# Iterate over each review in the corpus and add all bi-grams to the all_bigrams list\n",
    "for review in df['Corpus_Reviews']:\n",
    "    bigrams = ngrams(review, 2)\n",
    "    all_bigrams.extend(list(bigrams))\n",
    "\n",
    "# Count the frequency of each bi-gram and get the 10 most common bi-grams\n",
    "bigram_freq = Counter(all_bigrams)\n",
    "top_bigrams = bigram_freq.most_common(15)\n",
    "\n",
    "# Print the 15 most common bi-grams\n",
    "print(\"The 15 most frequent bi-grams in the corpus are:\")\n",
    "for bigram, freq in top_bigrams:\n",
    "    print(f\"{bigram}: {freq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the tuples to strings\n",
    "top_bigrams_str = [f\"{bigram[0]} {bigram[1]}\" for bigram, freq in top_bigrams]\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "ax = sns.barplot(x=[val[1] for val in top_bigrams], y=top_bigrams_str, color='r')\n",
    "ax.set_xlabel('Frequency')\n",
    "ax.set_ylabel('Bi-gram')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tri-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to store all tri-grams in the corpus\n",
    "all_trigrams = []\n",
    "\n",
    "# Iterate over each review in the corpus and add all tri-grams to the all_trigrams list\n",
    "for review in df['Corpus_Reviews']:\n",
    "    trigrams = ngrams(review, 3)\n",
    "    all_trigrams.extend(list(trigrams))\n",
    "    \n",
    "# Count the frequency of each tri-gram and get the 10 most common tri-grams\n",
    "trigram_freq = Counter(all_trigrams)\n",
    "top_trigrams = trigram_freq.most_common(15)\n",
    "\n",
    "# Print the 15 most common tri-grams\n",
    "print(\"The 15 most frequent tri-grams in the corpus are:\")\n",
    "for trigram, freq in top_trigrams:\n",
    "    print(f\"{trigram}: {freq}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "ax = sns.barplot(x=[val[1] for val in top_trigrams], y=[f\"{val[0][0]} {val[0][1]} {val[0][2]}\" for val in top_trigrams], color='r')\n",
    "ax.set_xlabel('Frequency')\n",
    "ax.set_ylabel('Tri-gram')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 15 Bad Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ' '.join(clean_text)  # join the list of strings using a space as separator\n",
    "\n",
    "# Initialize the sentiment analyzer\n",
    "analyzer = vader.SentimentIntensityAnalyzer()\n",
    "\n",
    "# Tokenize the text into individual words\n",
    "words = word_tokenize(text)\n",
    "\n",
    "# Get the negative words\n",
    "negative_words = []\n",
    "for word in tqdm(words):\n",
    "    score = analyzer.polarity_scores(word)['neg']\n",
    "    if score > 0.5:  # Threshold for negative sentiment\n",
    "        negative_words.append(word)\n",
    "        \n",
    "word_freq = Counter(negative_words)\n",
    "top_words = word_freq.most_common(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "ax = sns.barplot(x=[val[1] for val in top_words], y=[val[0] for val in top_words], color='r')\n",
    "ax.set_xlabel('Frequency')\n",
    "ax.set_ylabel('Bad Word')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi-gram of Bad Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bi-gram of the words\n",
    "bigrams = ngrams(words, 2)\n",
    "\n",
    "# Convert the bigrams into strings\n",
    "bigram_strings = [' '.join(bigram) for bigram in bigrams]\n",
    "\n",
    "# Get the negative bigrams\n",
    "negative_bigrams = []\n",
    "for bigram in tqdm(bigram_strings):\n",
    "    score = analyzer.polarity_scores(bigram)['neg']\n",
    "    if score > 0.5:  # Threshold for negative sentiment\n",
    "        negative_bigrams.append(bigram)\n",
    "bigram_freq = Counter(negative_bigrams)\n",
    "top_bigrams = bigram_freq.most_common(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "ax = sns.barplot(x=[val[1] for val in top_bigrams], y=[val[0] for val in top_bigrams], color='r')\n",
    "ax.set_xlabel('Frequency')\n",
    "ax.set_ylabel('Bad Bigram')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tri-gram of Bad Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tri-grams\n",
    "trigrams = ngrams(words, n=3)\n",
    "\n",
    "# Convert the trigrams into strings\n",
    "trigram_strings = [' '.join(trigram) for trigram in trigram]\n",
    "\n",
    "# Get the negative words\n",
    "negative_trigrams = []\n",
    "for trigram in tqdm(trigrams):\n",
    "    score = analyzer.polarity_scores(' '.join(trigram))['neg']\n",
    "    if score > 0.5:  # Threshold for negative sentiment\n",
    "        negative_trigrams.append(' '.join(trigram))\n",
    "        \n",
    "        \n",
    "# Get the top 15 bad tri-negative_trigrams\n",
    "trigram_freq = Counter(negative_trigrams)\n",
    "top_trigram = trigram_freq.most_common(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "ax = sns.barplot(x=[val[1] for val in top_trigram], y=[val[0] for val in top_trigram], color='r')\n",
    "ax.set_xlabel('Frequency')\n",
    "ax.set_ylabel('Bad Tri-gram')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepering for model\n",
    "\n",
    "It's apparent that the reviews vary in lengths, which is problematic. Neural networks cannot accept data of different lengths. Therefore, it is imperative that we standardize the length of each review. To accomplish this, we need to:\n",
    "\n",
    "* Split data into train test\n",
    "* Determine a fxied number of word.\n",
    "* Tokenize the words.\n",
    "* Pad the text.\n",
    "* One-hot encode Y\n",
    "* Reshape data from 2D to 3D"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data into Train Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning review text which will then be saved to x\n",
    "\n",
    "x = []\n",
    "\n",
    "sentences = list(df['Review Text'])\n",
    "for sen in sentences:\n",
    "    x.append(preprocess_text(sen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x = df['Review Text']\n",
    "x = np.array(x)\n",
    "y = df['Positive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20)\n",
    "\n",
    "print(\"shape of x_train:\", x_train.shape)\n",
    "print(\"shape of x_test:\", x_test.shape)\n",
    "\n",
    "print(\"shape of y_train:\", y_train.shape)\n",
    "print(\"shape of y_test:\", y_test.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine Number of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking mean and std\n",
    "df[\"Word_count\"].describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(df[\"Word_count\"], height=8, aspect=1.5, bins=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 10))\n",
    "sns.boxplot(df[\"Word_count\"])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon careful examination of both visuals, it becomes apparent that around 350 words represents the optimal maximum word count."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IQR Method\n",
    "To ensure accuracy, I will also use the IQR method to verify the validity or proximity of my observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = df[\"Word_count\"].quantile(0.25)\n",
    "q3 = df[\"Word_count\"].quantile(0.75)\n",
    "iqr = q3-q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_limit = q3 + (1.5 * iqr)\n",
    "lower_limit = q1 - (1.5 * iqr)\n",
    "\n",
    "upper_limit, lower_limit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Check this comment\n",
    "\n",
    "The upper limit, which is 307.5, closely aligns with my observation. Therefore, I will round it up to 350."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing the text. Creates a word-to-index dictionary\n",
    "\n",
    "Once a word is tokenized, the assigned number represents its frequency within the text. The number 1 corresponds to the most frequently occurring word, and so forth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is what the words look like before getting tokenized\n",
    "print(x_train[4])\n",
    "print(y_train[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Let's check what our vocabulary size is\n",
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(x_train.copy())\n",
    "\n",
    "VOCAB_SIZE = len(tokenizer.word_index)\n",
    "print(\"Vocabulary size:\", VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 52234\n",
    "\n",
    "#, lower=True\n",
    "tokenizer = Tokenizer(VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "\n",
    "# Providing each token an integer\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_test = tokenizer.texts_to_sequences(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is what the words look like after beeing tokenized\n",
    "print(x_train[4])\n",
    "print(y_train[4])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding the text\n",
    "\n",
    "* If a review exceeds 350 words, we will remove the excess words.\n",
    "* If a review contains less than 350 words, we will append zeros to it until it reaches a length of 750 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word lenghts before padding\n",
    "len(x_train[0]), len(x_train[1]), len(x_train[2]), len(x_train[3]), len(x_train[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Max length of words\n",
    "MAXLEN = 350"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = keras.utils.pad_sequences(x_train, padding='post', maxlen=MAXLEN)\n",
    "x_test = keras.utils.pad_sequences(x_test, padding='post', maxlen=MAXLEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word lenghts after padding\n",
    "len(x_train[0]), len(x_train[1]), len(x_train[2]), len(x_train[3]), len(x_train[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And this is what it looks like if we take a closer look\n",
    "x_train[4]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot Encoding on y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The assigned number is dependent on the type of rating being used, which is currently set to 0 or 1 in this instance.\n",
    "# However, it is possible to modify this.\n",
    "num_classes = 2\n",
    "\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "\n",
    "print(y_train.shape)\n",
    "print(y_test[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resaping the data from 2D to 3D\n",
    "\n",
    "Recurrent Neural Networks (RNNs) require 3D input because they are designed to operate on sequences of data. The 3D input tensor consists of a batch size, sequence length, and feature dimensions.\n",
    "\n",
    "* Batch size: The number of sequences in a batch, typically represented as the first dimension of the 3D tensor.\n",
    "* Sequence length: The number of time steps in each sequence, represented as the second dimension of the 3D tensor.\n",
    "* Feature dimensions: The number of features or variables in each time step, represented as the third dimension of the 3D tensor.\n",
    "\n",
    "The 3D input tensor allows RNNs to process a sequence of data over time, where each time step contains a set of features. By using a 3D input, RNNs can learn and remember patterns and dependencies across multiple time steps, which is useful in a variety of applications such as natural language processing, speech recognition, and time series forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array(x_train).reshape((x_train.shape[0], x_train.shape[1], 1))\n",
    "x_test = np.array(x_test).reshape((x_test.shape[0], x_test.shape[1], 1))\n",
    "\n",
    "print(x_train.shape)\n",
    "print(\"Sentences:\", x_train.shape[0], \"Words:\", x_train.shape[1], \"TimeSteps:\", x_train.shape[2], \"\\n\")\n",
    "print(x_test.shape)\n",
    "print(\"Sentences:\", x_test.shape[0], \"Words:\", x_test.shape[1], \"TimeSteps:\", x_test.shape[2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN Model\n",
    "model = tf.keras.Sequential([\n",
    "tf.keras.layers.Embedding(input_dim=VOCAB_SIZE, output_dim=50, input_length=MAXLEN),\n",
    "tf.keras.layers.LSTM(32, return_sequences=True),\n",
    "tf.keras.layers.Dropout(0.2),\n",
    "tf.keras.layers.LSTM(16, return_sequences=True),\n",
    "tf.keras.layers.Dropout(0.2),\n",
    "tf.keras.layers.Flatten(),\n",
    "tf.keras.layers.Dense(num_classes , activation=\"sigmoid\"),\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights\n",
    "\n",
    "We are dealing with imbalanced data. It's important to adjust the weights of the classes to reflect their distribution. This can be done by calculating the class weights and passing them to the model during training.\n",
    "\n",
    "The class weights can be calculated using various techniques such as inverse frequency, inverse square root frequency, or user-defined weights. Inverse frequency assigns a weight to each class that is inversely proportional to the number of samples in that class. Inverse square root frequency assigns a weight that is the inverse square root of the frequency of the class. User-defined weights allow the user to manually assign weights to each class based on their domain knowledge.\n",
    "\n",
    "I opted for inverse frequency as you can seee below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency\n",
    "freq = pd.value_counts(df['Positive'])\n",
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverse frequency\n",
    "weights = {0: freq.sum() / freq[0], 1: freq.sum() / freq[1]}\n",
    "weights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating checkpoint that saves the best model\n",
    "checkpoint = ModelCheckpoint('model/', save_best_only=True)\n",
    "#, monitor='val_accuracy', mode='max'\n",
    "\n",
    "# Stop the training if the val_loss does not improve on next 4 epochs\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=4, verbose=1)\n",
    "\n",
    "# Creates a csv file and saves the model metrics in it\n",
    "log_csv = CSVLogger('my_logs.csv', separator=',', append=False)\n",
    "\n",
    "# A lsit with the 3 instances that will be put into callbacks in model.fit()\n",
    "callback_lsit = [checkpoint, early_stop, log_csv]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an instance of BinaryCrossentropy to pass into model.compile()\n",
    "loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "# Configure the model for training.\n",
    "model.compile(optimizer=Adam(learning_rate=(0.0001)), loss=loss_fn, metrics=['accuracy', AUC(name='AUC')])\n",
    "\n",
    "# Train a model\n",
    "history = model.fit(x_train, y_train, batch_size=150, epochs=20, validation_split=0.20, class_weight=weights, callbacks=[callback_lsit])\n",
    "\n",
    "# print the best epoch\n",
    "print(f\"The best epoch is {early_stop.best_epoch}.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "\n",
    "Let's evaluate the model to see how well it did"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = load_model('model/')\n",
    "\n",
    "test_predictions = (best_model.predict(x_test) > 0.5).astype(int)\n",
    "print(classification_report(y_test, test_predictions, zero_division=0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The precision and recall for both classes are high, indicating that the model is able to correctly identify both positive and negative cases. The f1-score is also high for both classes, which suggests that the model has a good balance between precision and recall.\n",
    "\n",
    "The macro and weighted average scores are also high, which indicates that the model is performing well overall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Performance Chart\n",
    "\n",
    "# accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "# Add a marker on best epoch\n",
    "plt.axvline(x=early_stop.best_epoch, color='r')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right')\n",
    "\n",
    "# Add a marker on best epoch\n",
    "plt.axvline(x=early_stop.best_epoch, color='r')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# AUC\n",
    "plt.plot(history.history['AUC'])\n",
    "plt.plot(history.history['val_AUC'])\n",
    "\n",
    "plt.title('model auc')\n",
    "plt.ylabel('AUC')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "# Add a marker on best epoch\n",
    "plt.axvline(x=early_stop.best_epoch, color='r')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: make this a comment instead with f string that gets the score and round it\n",
    "\n",
    "\n",
    "Looking at this output, the accuracy score for the training set is around 89-90%, while the accuracy score for the validation set is around 90-91%. This suggests that the model is performing well and not overfitting too much to the training data. The loss scores are also decreasing with each epoch, which is a good sign that the model is learning from the data.\n",
    "\n",
    "\n",
    "Great! The ROC AUC score of my model is 0.901998439517365 which indicates that my model is performing well. A value of 0.5 indicates random guessing, while a value of 1 indicates perfect performance. A score between 0.7 to 0.9 is considered good, while a score above 0.9 is considered excellent. Therefore, I would say my model is performing well in terms of its ability to distinguish between positive and negative samples."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Mtrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Check comment.\n",
    "# Get the predicted labels for the test set\n",
    "#y_pred = model.predict(x_test)\n",
    "#y_pred = (y_pred > 0.5)\n",
    "\n",
    "# Create the confusion matrix\n",
    "cm = confusion_matrix(y_test.argmax(axis=1), test_predictions.argmax(axis=1))\n",
    "\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, cmap='Blues', fmt='g')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: make this a comment instead with f string that gets the score and rounds it\n",
    "\n",
    "\n",
    "Based on the output, the confusion matrix has four values - true negatives (TN), false positives (FP), false negatives (FN), and true positives (TP). The matrix is a 2x2 matrix, with the top row representing the actual negatives and the bottom row representing the actual positives. The left column represents the predicted negatives and the right column represents the predicted positives.\n",
    "\n",
    "Here, the model predicted 321 true negatives and 4705 true positives. It also predicted 276 false positives and 164 false negatives."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Predictions\n",
    "\n",
    "Next, we will utilize our neural network to predict the sentiment of our own reviews. However, since the reviews are encoded, we need to convert our new reviews into the same encoding format so that the network can process them. To accomplish this, we will load the encodings from the existing dataset and apply them to our new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "data = initial_processing_df\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# Empty list where the cleaned text will be stored\n",
    "data_clean_text = []\n",
    "\n",
    "data_clean_sentences = list(data['Review'])\n",
    "for sen in tqdm(data_clean_sentences):\n",
    "    data_clean_text.append(preprocess_text(sen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "data_tokenized = tokenizer.texts_to_sequences(data_clean_text)\n",
    "data_padded = keras.utils.pad_sequences(data_tokenized, padding='post', maxlen=MAXLEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "best_model = load_model('model/')\n",
    "\n",
    "# Use the model to make predictions\n",
    "data_predictions = best_model.predict(data_padded)\n",
    "\n",
    "# Get the predicted class labels\n",
    "data_class_predictions = np.argmax(data_predictions, axis=1)\n",
    "\n",
    "# Create a DataFrame with the predicted class labels\n",
    "data_predictions_df = pd.DataFrame(data_class_predictions, columns=['LSTM_Prediction_Positive'])\n",
    "\n",
    "# Concatenate the original DataFrame with the new predictions DataFrame\n",
    "data = pd.concat([data, data_predictions_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's evaluate the model to see how well it did\n",
    "print(classification_report(data['Positive'], data['LSTM_Prediction_Positive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# function to compare values in two columns and return boolean\n",
    "def check_cols(row):\n",
    "    return row['Positive'] == row['LSTM_Prediction_Positive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# apply function to each row using the apply method\n",
    "data['same_values'] = data.apply(check_cols, axis=1)\n",
    "\n",
    "data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the percentage of true and false values, we need to divide the count of each by the total count and multiply by 100.Assuming the count is for a single column, we can calculate the percentages as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "same_values = data['same_values'].value_counts()\n",
    "same_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "print(f'Percentage of true values: {same_values[0] / (same_values[0] + same_values[1]) * 100:.2f}%')\n",
    "print(f'Percentage of false values: {same_values[1] / (same_values[0] + same_values[1]) * 100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
